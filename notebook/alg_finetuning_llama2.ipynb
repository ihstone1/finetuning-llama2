{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "aee8c99b-1504-4e16-be8b-c85e47bfd46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "9738435b-fad6-499e-a732-57e84bd1e5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import re\n",
    "import os, torch, logging\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM, TrainingArguments, pipeline, HfArgumentParser\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "2ff5da24-2a5e-4743-a1a4-b737efdc9242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_papers():\n",
    "    \"\"\"Fetches papers from the arXiv API and returns them as a list of strings.\"\"\"\n",
    "    url = 'http://export.arxiv.org/api/query?search_query=ti:llama&start=0&max_results=70'\n",
    "    response = urllib.request.urlopen(url)\n",
    "    data = response.read().decode('utf-8')\n",
    "    root = ET.fromstring(data)\n",
    "\n",
    "    papers_list = []\n",
    "    for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):\n",
    "        title = entry.find('{http://www.w3.org/2005/Atom}title').text\n",
    "        summary = entry.find('{http://www.w3.org/2005/Atom}summary').text\n",
    "        paper_info = f\"Title: {title}\\nSummary: {summary}\\n\"\n",
    "        papers_list.append(paper_info)\n",
    "\n",
    "    return papers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "55196f7f-b1a7-4c3a-8bb9-34b02ccef272",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    paper_list = fetch_papers()\n",
    "except:\n",
    "    paper_list = pd.read_csv('df.csv')\n",
    "    paper_list = list(paper_list[paper_list.columns.values[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "06c7205f-7a4d-4a2f-aab5-0bc4fc676c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'Title:','<s>[INST]',text)\n",
    "    text = re.sub(r'Summary:','[/INST]',text)\n",
    "    text = re.sub(r'\\n\\n','',text)\n",
    "    text = re.sub(r'\\n',' ',text)\n",
    "    text = re.sub(r\"http\\S+\",\"\",text)\n",
    "    text = re.sub(r\"http\\S+\",\"\",text)\n",
    "    text = re.sub(r'@[^\\s]+',\"\",text)\n",
    "    text = re.sub(r'\\s+',\" \",text)\n",
    "    text += ' </s>'\n",
    "    return re.sub(r'\\^[^ ]+',\"\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "88eae72a-7267-4736-97a2-8fbcf2cd320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(paper_list)):\n",
    "    paper_list[i] = clean_text(paper_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a269b23a-eac4-40b7-b01b-dd0f8cc227f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_df = pd.DataFrame({'Text':paper_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a1bd1d3a-7391-4a0e-a678-2cefdb28f3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-trained model name\n",
    "base_model_name = \"NousResearch/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b1f3d377-0f70-4b98-b17e-2d9cb3f7b514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download vocab from huggingface\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "a4ce4bc2-efc3-4276-ba16-4ceb73b0deac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization Config\n",
    "quant_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                  bnb_4bit_quant_dtype=\"nf4\", # to load model in 4bit using NF4 quantization\n",
    "                                  bnb_4bit_compute_dtype=torch.bfloat16, # forward / backward pass can be in 16, 32. (bnb_4bit_compute_type=torch.float16.... difference?)\n",
    "                                  bnb_4bit_use_double_quant=False # if true, uses second quantization to save an additional 0.4 bits per param\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891f03f9-9fbc-4ace-8dce-bf8ba5b6985a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51692f26c6524aea8ba0f128d5bd5a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the base model\n",
    "# Will fail if no GPU\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config = quant_config,\n",
    "    device_map = {\"\": 0},\n",
    "    use_safetensors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf4e549-e913-49b4-b096-f63339adc629",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.config.use_cache = False\n",
    "\n",
    "# setting to value diff than 1 -> more accurate but slower computation of linear layers\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0320f0ae-af7c-44e7-8dc9-3a4d53486c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA Config\n",
    "peft_parameters = LoraConfig(\n",
    "    lora_alpha=8, # experiment with different values\n",
    "    lora_dropout=0.1,\n",
    "    r=8, # experiment with different values\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules = [\"q_proj\",\"k_proj\",\"v_proj\"]\n",
    "    # [\n",
    "      #  \"dense\",\n",
    "       # \"dense_h_to_4h\",\n",
    "        #\"dense_4h_to_h\" ]\n",
    "     #uncomment for maximum performance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8c8919-7540-4159-98b7-c6252e9fcc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Params\n",
    "train_params = TrainingArguments(\n",
    "    output_dir=\"./results_modified\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=25,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4c7b43-d765-452c-b28c-6d9603ec2b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_hf = Dataset.from_pandas(paper_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e387994e-c263-4b44-8303-40bfe83d905d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer\n",
    "fine_tuning = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=paper_hf,\n",
    "    peft_config=peft_parameters, # without this arg, we finetune entire base model\n",
    "    dataset_text_field=\"Text\",\n",
    "    tokenizer=llama_tokenizer,\n",
    "    args=train_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6240826a-d55e-4409-849f-831ec732570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"For which tasks has Llama-2 already been used successfully?\"\n",
    "text_gen = pipeline(task=\"text-generation\", model=base_model, tokenizer=llama_tokenizer, max_length=200)\n",
    "output = text_gen(f\"<s>[INST] {query} [/INST]\")\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c901460b-d981-42c1-8b1d-b35de4c1a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e11680-9fe7-4c7c-907d-69fdd5f5c046",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()+'/'+'llama-7b-alg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd14f05c-5f02-4cdc-9994-c8c3a8021846",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning.model.save_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a4e5c5-792d-448a-8cd5-5913e4b67171",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_peft = PeftModel.from_pretrained(base_model, path)\n",
    "model_peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210151c8-a933-43f2-b09d-36224629b231",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"For which tasks has Llama-2 already been used successfully?\"\n",
    "text_gen = pipeline(task=\"text-generation\", model=model_peft, tokenizer=llama_tokenizer, max_length=200)\n",
    "output = text_gen(f\"<s>[INST] {query} [/INST]\")\n",
    "print(output[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
