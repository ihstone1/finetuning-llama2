{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aee8c99b-1504-4e16-be8b-c85e47bfd46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8f8133-e6bf-44bc-8f8f-75c070f1c0ed",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9738435b-fad6-499e-a732-57e84bd1e5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import re\n",
    "import os, torch, logging\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM, TrainingArguments, pipeline, HfArgumentParser\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3c0188-03c2-48ee-89bf-09b7f6a339fc",
   "metadata": {},
   "source": [
    "## Fetch training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ff5da24-2a5e-4743-a1a4-b737efdc9242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_papers():\n",
    "    \"\"\"Fetches papers from the arXiv API and returns them as a list of strings.\"\"\"\n",
    "    url = 'http://export.arxiv.org/api/query?search_query=ti:llama&start=0&max_results=70'\n",
    "    response = urllib.request.urlopen(url)\n",
    "    data = response.read().decode('utf-8')\n",
    "    root = ET.fromstring(data)\n",
    "\n",
    "    papers_list = []\n",
    "    for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):\n",
    "        title = entry.find('{http://www.w3.org/2005/Atom}title').text\n",
    "        summary = entry.find('{http://www.w3.org/2005/Atom}summary').text\n",
    "        paper_info = f\"Title: {title}\\nSummary: {summary}\\n\"\n",
    "        papers_list.append(paper_info)\n",
    "\n",
    "    return papers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55196f7f-b1a7-4c3a-8bb9-34b02ccef272",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    paper_list = fetch_papers()\n",
    "except:\n",
    "    paper_list = pd.read_csv('df.csv')\n",
    "    paper_list = list(paper_list[paper_list.columns.values[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1681f559-6a01-47dd-8c3b-2ae86a982c22",
   "metadata": {},
   "source": [
    "## Clean/prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06c7205f-7a4d-4a2f-aab5-0bc4fc676c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'Title:','<s>[INST]',text)\n",
    "    text = re.sub(r'Summary:','[/INST]',text)\n",
    "    text = re.sub(r'\\n\\n','',text)\n",
    "    text = re.sub(r'\\n',' ',text)\n",
    "    text = re.sub(r\"http\\S+\",\"\",text)\n",
    "    text = re.sub(r\"http\\S+\",\"\",text)\n",
    "    text = re.sub(r'@[^\\s]+',\"\",text)\n",
    "    text = re.sub(r'\\s+',\" \",text)\n",
    "    text += ' </s>'\n",
    "    return re.sub(r'\\^[^ ]+',\"\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88eae72a-7267-4736-97a2-8fbcf2cd320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(paper_list)):\n",
    "    paper_list[i] = clean_text(paper_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a269b23a-eac4-40b7-b01b-dd0f8cc227f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_df = pd.DataFrame({'Text':paper_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5d1829a-c2c7-477b-9300-6db4f910913e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas -> hugging face\n",
    "paper_hf = Dataset.from_pandas(paper_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbef469-0dda-4f5d-83d2-bcf00a94b57b",
   "metadata": {},
   "source": [
    "## Load base mdel (llama-2 7B params)\n",
    "In order to load the base model (Llama 2 7B), we need the following:\n",
    "1. Tokenizer\n",
    "2. Quantization config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1bd1d3a-7391-4a0e-a678-2cefdb28f3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-trained model name\n",
    "base_model_name = \"NousResearch/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1f3d377-0f70-4b98-b17e-2d9cb3f7b514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download vocab from huggingface\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "llama_tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a1634f-5947-4ef5-a015-fa80fa4136df",
   "metadata": {},
   "source": [
    "As per QLoRA, we load the model in 4bit (`load_in_4bit=True`), specifically using NF4 quantization (`bnb_4bit_quant_dtype=\"nf4\"`). Our model would suffer greatly from information loss if we were to keep the parameters in 4bit. For this reason, we compute in 16bit (`bnb_4bit_compute_dtype=torch.bfloat16`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4ce4bc2-efc3-4276-ba16-4ceb73b0deac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization Config\n",
    "quant_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                  bnb_4bit_quant_dtype=\"nf4\", # to load model in 4bit using NF4 quantization\n",
    "                                  bnb_4bit_compute_dtype=torch.bfloat16, # forward / backward pass can be in 16, 32. (bnb_4bit_compute_type=torch.float16.... difference?)\n",
    "                                  bnb_4bit_use_double_quant=False # if true, uses second quantization to save an additional 0.4 bits per param\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "891f03f9-9fbc-4ace-8dce-bf8ba5b6985a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847cefe5285949b7beaac465d0027ace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the base model\n",
    "# Will fail if no GPU\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config = quant_config,\n",
    "    device_map = {\"\": 0},\n",
    "    use_safetensors=True\n",
    ")\n",
    "\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "# setting to value diff than 1 -> more accurate but slower computation of linear layers\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c75ef4-8984-4023-848c-aad755a69410",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "In order to finetune with LoRA, we first need to set the config. Two hyper-parameters worth experimenting with are `lora_alpha` and `r`.\n",
    "\n",
    "`r` is the rank for our decomposed matrices. For example, suppose the original weight matrix in our base model has dimensions 100x100. Then, if we go with `r = 8`, our A and B matrices will be 100x8 and 8x100.\n",
    "\n",
    "`lora_alpha` ($\\alpha$\\) is used to scale our learned weight matrix, by a factor of $\\dfrac{\\alpha}{r}$. By assigning $\\alpha$ at a value greater (less) than our rank $r$, we are in effect putting more (less) importance of our learned weights than the original weights. Note that this can also be achieved by changing the learning rate with a fixed $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0320f0ae-af7c-44e7-8dc9-3a4d53486c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA Config\n",
    "peft_parameters = LoraConfig(\n",
    "    lora_alpha=8, # experiment with different values\n",
    "    lora_dropout=0.1,\n",
    "    r=8, # experiment with different values\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules = [\"q_proj\",\"k_proj\",\"v_proj\"]\n",
    "    # [\n",
    "      #  \"dense\",\n",
    "       # \"dense_h_to_4h\",\n",
    "        #\"dense_4h_to_h\" ]\n",
    "     #uncomment for maximum performance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a8c8919-7540-4159-98b7-c6252e9fcc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Params\n",
    "train_params = TrainingArguments(\n",
    "    output_dir=\"./results_modified\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=25,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e387994e-c263-4b44-8303-40bfe83d905d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "/opt/app-root/lib64/python3.9/site-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf55fa459e14a38b86870e57a370cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/59 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Trainer\n",
    "fine_tuning = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=paper_hf,\n",
    "    peft_config=peft_parameters, # without this arg, we finetune entire base model\n",
    "    dataset_text_field=\"Text\",\n",
    "    tokenizer=llama_tokenizer,\n",
    "    args=train_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c901460b-d981-42c1-8b1d-b35de4c1a7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/opt/app-root/lib64/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:29, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15, training_loss=2.718391927083333, metrics={'train_runtime': 33.3368, 'train_samples_per_second': 1.77, 'train_steps_per_second': 0.45, 'total_flos': 428732317827072.0, 'train_loss': 2.718391927083333, 'epoch': 1.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "fine_tuning.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28e11680-9fe7-4c7c-907d-69fdd5f5c046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "path = os.getcwd()+'/'+'llama-7b-alg'\n",
    "fine_tuning.model.save_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62a4e5c5-792d-448a-8cd5-5913e4b67171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_peft = PeftModel.from_pretrained(base_model, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328c86e9-93da-4db1-9d00-ccc5ee59fe6d",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Let's ask our model a question and see its response, both from the base model as well as our finetuned model. Note that `PeftModel(base_model,path_to_peft_config)` converts base_model architecture to peft architecture. So we need to reload base_model to rever it to its original state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65a5f04e-9550-4799-aee2-e3b29974890d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84dc50a7c8cd43d08f96f8ddc5438c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reload the base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config = quant_config,\n",
    "    device_map = {\"\": 0},\n",
    "    use_safetensors=True\n",
    ")\n",
    "\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "# setting to value diff than 1 -> more accurate but slower computation of linear layers\n",
    "base_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa397192-fc77-4048-aee0-120f7cf906e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "210151c8-a933-43f2-b09d-36224629b231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "/opt/app-root/lib64/python3.9/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] For which tasks has Llama-2 already been used successfully? [/INST]  Llama-2 has been successfully used for a variety of natural language processing (NLP) tasks, including but not limited to:\n",
      " nobody.\n",
      "\n",
      "1. Text classification: Llama-2 has been used to classify text into different categories such as positive/negative sentiment, topic classification, and spam detection.\n",
      "2. Named entity recognition: Llama-2 has been used to extract named entities such as people, organizations, and locations from text.\n",
      "3. Part-of-speech tagging: Llama-2 has been used to assign part-of-speech tags to words in a sentence, such as noun, verb, adjective, etc.\n",
      "4. Dependency parsing: Llama-2 has been used to analyze the grammatical structure of a sentence and identify the relationships between words\n"
     ]
    }
   ],
   "source": [
    "query = \"For which tasks has Llama-2 already been used successfully?\"\n",
    "text_gen = pipeline(task=\"text-generation\", model=base_model, tokenizer=llama_tokenizer, max_length=200)\n",
    "output = text_gen(f\"<s>[INST] {query} [/INST]\")\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef7a8608-3240-4e2e-bd90-811b51f8d903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear4bit(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): Linear4bit(\n",
       "                in_features=4096, out_features=4096, bias=False\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00fda12f-4631-420d-8301-12931ac95cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] For which tasks has Llama-2 already been used successfully? [/INST]  Llama-2 has been used successfully for a wide range of natural language processing (NLP) tasks, including but not limited to:\n",
      " Unterscheidung von Textsorten: Llama-2 has been used to classify text into different categories, such as news articles, social media posts, and product reviews.\n",
      "\n",
      " Sentiment Analysis: Llama-2 has been used to analyze the sentiment of text, such as determining whether a piece of text is positive, negative, or neutral.\n",
      "\n",
      " Named Entity Recognition: Llama-2 has been used to identify and classify named entities in text, such as people, organizations, and locations.\n",
      "\n",
      " Part-of-Speech Tagging: Llama-2 has been used to assign part-of-speech tags to words in text, such as determining whether\n"
     ]
    }
   ],
   "source": [
    "query = \"For which tasks has Llama-2 already been used successfully?\"\n",
    "text_gen = pipeline(task=\"text-generation\", model=model_peft, tokenizer=llama_tokenizer, max_length=200)\n",
    "output = text_gen(f\"<s>[INST] {query} [/INST]\")\n",
    "print(output[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
